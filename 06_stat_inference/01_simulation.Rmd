---
title: "Statistical Inference: Simulation Exercise"
output: pdf_document
author: https://github.com/johnsug/dss/blob/master/06_stat_inference/01_simulation.Rmd
---

# Overview

In this exercise, I will demonstrate how the exponential distribution approaches a normal distribution by means of the Central Limit Theorem. Quoting [Wikipedla](https://en.wikipedia.org/wiki/Central_limit_theorem),

> In probability theory, the central limit theorem (CLT) states that, given certain conditions, the arithmetic mean of a sufficiently large number of iterates of independent random variables, each with a well-defined expected value and well-defined variance, will be approximately normally distributed, regardless of the underlying distribution.

In order to demonstrate the Central Limit Thereom, I will: 

1. Simulate 40 exponential random numbers (with $\lambda$=0.2)
2. Record the mean
3. Repeat the exercise 1000 times

# Simulations

My code to simulate the expoenential random numbers is as follows:

```{r simulations}
set.seed(40)                              ## set random number seed (for reproducibility)
means <- vector()                         ## initialize empty vector to store means
for(i in 1:1000){                         ## initialize simulation loop
  means[i] <- mean(rexp(40, rate=0.2))    ## generate, and take the mean of, 40 exponential 
}                                         ## random numbers with rate = 0.2
```

# Sample Mean vs Theoretical Mean

The theoretical mean of an exponential distribution is 1/$\lambda$. In this case, that would be 1/.2 or **`r 1/.2`**.

The sample mean from the simulations is **`r round(mean(means),3)`**.

The sample and theoretical means are nearly identical.

# Sample Variance vs Theoretical Variance

The theoretical variance of an exponential distribution is (1/$\lambda$)$^2$. In this case, this would be (1/.2)$^2$ or **`r (1/.2)^2`**. However, we are _sampling_ from the exponential distribution. Therefore, the theoretical sample variance is (1/$\lambda$)$^2$ / $\sqrt{n}$ or (1/.2)$^2$ / $\sqrt{1000}$ or **`r round(1/(.2^2)/sqrt(1000),3)`**.

The sample variance of the simulations is **`r round(var(means),3)`**.

The two variances are similar but differ by **`r round(1/(.2^2)/sqrt(1000),3) - round(var(means),3)`**.

# Distribution

After describing the summary statistics, we should visualize the distribution of the sampled means drawn from the exponential distributions.

```{r histogram, fig.height=4}
library(ggplot2)
ggplot(data.frame(mean=means), aes(x=mean)) + xlim(2,8) + 
  geom_histogram(color="red", fill="orange", alpha=.4, binwidth=.2) + 
  labs(title="Histogram of Simulated Means", x="Simulated Mean", y="Count")
```

The histogram resembles a bell-shaped curve, suggesting that the means follow a normal distribution, however there is a slight rightward-skew.  To demonstrate how similar the data represents a normal distribution, I will overlay a normal distribution with the theoretical mean and variance indicated above ($\mu$=`r 1/.2`, $\sigma^2$=`r round(1/(.2^2)/sqrt(1000),3)`).

```{r density, fig.height=4}
## theoretical mean and variance
theo_mean <- 1/.2
theo_var <- round(1/(.2^2)/sqrt(1000),3)

## generate theoretical distribution
theo_dist <- data.frame(x=(200:800)/100)
theo_dist$y <- dnorm(theo_dist$x, mean=theo_mean, sd=sqrt(theo_var))

## plot histogram with overlayed density curve
library(ggplot2)
ggplot(data.frame(mean=means), aes(x=mean)) + 
  geom_histogram(color="red", fill="orange", alpha=.4, binwidth=.2) + 
  labs(title="Histogram of Simulated Means with Overlayed Normal Distribution") + 
  labs(x="Simulated Mean", y="Count") + 
  geom_line(data=theo_dist, aes(x=x, y=y*235), size=1, color="red")
```

As the chart above shows, the sampled means strongly resemble a normal distribution. However, statistical goodness-of-fit tests should be run in order to determine evaluate whether two samples come from a similar distribution.

# Goodness-of-Fit Tests

To evaluate whether the sampled means follow a normal distribution, we will employ the [Kolmogorovâ€“Smirnov ](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test) goodness-of-fit test. The null hypothesis of the KS test is that the sample is drawn from the reference distribution.

```{r ks}
ks <- ks.test(means, "pnorm", theo_mean, sqrt(theo_var))
ks
```

```{r reject, echo=FALSE}
reject <- ""
if(ks$p.value < 0.05) reject <- "fail to"
```

With a critical value of $\alpha$ = 0.05, the p-value from the KS test indicates that we should `r reject` reject the null hypothesis that that the sampled means follow a normal distribution (with p = `r round(ks$p.value,4)`).
